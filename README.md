# Azure Load Balancer - Lab P√©dagogique üéì

> **Objectif** : Prise en main d'Azure Load Balancer en d√©ployant une architecture 3-tiers avec r√©partition de charge

---

## üìñ Introduction

Ce projet est un **laboratoire pratique** pour comprendre et ma√Ætriser **Azure Load Balancer**. Vous allez d√©ployer une application web √† 3 couches (Frontend, Application, Data) avec des Load Balancers pour distribuer le trafic entre plusieurs serveurs.

### Ce que vous allez apprendre

- ‚úÖ Cr√©er et configurer des **Azure Load Balancers** (Public et Internal)
- ‚úÖ Utiliser des **Backend Pools** avec plusieurs VMs
- ‚úÖ Configurer des **Health Probes** pour la haute disponibilit√©
- ‚úÖ Mettre en place le **VNet Peering** entre r√©seaux virtuels
- ‚úÖ D√©ployer des VMs automatiquement avec **Cloud-Init**
- ‚úÖ Comprendre le **round-robin** et la r√©partition de charge
- ‚úÖ Configurer des **NSG (Network Security Groups)**
- ‚úÖ Utiliser un **Azure Bastion** pour se connecter aux VMs

### Architecture simplifi√©e

```
Internet
   ‚Üì
[Load Balancer Public] ‚Üí Frontend VMs (10.1.0.x:80 et 10.1.0.x:8443)
   ‚Üì
[Load Balancer Internal App] ‚Üí App VMs (10.2.0.x:5000 et 5001)
   ‚Üì
[Load Balancer Internal Data] ‚Üí Data VMs (10.3.0.x:6000 et 6001)
```

---

## üèóÔ∏è Architecture du projet

### Vue d'ensemble des couches

Le projet est compos√© de **3 couches isol√©es** dans des r√©seaux virtuels s√©par√©s :

#### 1Ô∏è‚É£ **Couche Frontend** (front-vnet: 10.1.0.0/16)

**R√¥le** : Servir l'interface utilisateur et recevoir les requ√™tes HTTP depuis Internet

**VMs d√©ploy√©es** :
- `frontend-vm1` (10.1.0.4:80) - Port 80 standard
- `frontend-vm2` (10.1.0.5:8443) - Port 8443 s√©curis√©
- `frontend-vm2_b` (10.1.0.21:8443) - VM test suppl√©mentaire

**Load Balancer** : `front-lb` (Public IP) - Distribue le trafic HTTP/HTTPS

**Ports expos√©s** :
- Port 80 ‚Üí Rout√© vers frontend-vm1
- Port 8443 ‚Üí Rout√© vers frontend-vm2 et frontend-vm2_b (round-robin)

**Endpoints disponibles** :
- `GET /` - Interface HTML
- `GET /whoami` - Informations sur l'instance
- `GET /api` - Appelle la couche App (traverse toute la stack)
- `GET /health` - Health check
- `GET /probe/app` - Interroge la couche App
- `GET /probe/data` - Interroge la couche Data

#### 2Ô∏è‚É£ **Couche Application** (app-vnet: 10.2.0.0/16)

**R√¥le** : Traiter la logique m√©tier et faire le pont entre Frontend et Data

**VMs d√©ploy√©es** :
- `app-vm1` (10.2.0.4:5000)
- `app-vm2` (10.2.0.5:5001)
- `app-vm2_b` (10.2.0.21:5002)

**Load Balancer** : `app-lb` (IP interne 10.2.0.250) - Distribue entre les VMs app

**Ports** :
- 5000 ‚Üí Rout√© vers app-vm1
- 5001 ‚Üí Rout√© vers app-vm2
- 5002 ‚Üí Rout√© vers app-vm2_b (test)

**Endpoints disponibles** :
- `GET /whoami` - Informations sur l'instance
- `GET /api` - Appelle la couche Data
- `GET /health` - Health check

#### 3Ô∏è‚É£ **Couche Data** (data-vnet: 10.3.0.0/16)

**R√¥le** : Simuler une base de donn√©es ou un backend

**VMs d√©ploy√©es** :
- `data-vm1` (10.3.0.4:6000)
- `data-vm2` (10.3.0.5:6001)
- `data-vm2_b` (10.3.0.21:6002)

**Load Balancer** : `data-lb` (IP interne 10.3.0.250) - Distribue entre les VMs data

**Ports** :
- 6000 ‚Üí Rout√© vers data-vm1
- 6001 ‚Üí Rout√© vers data-vm2
- 6002 ‚Üí Rout√© vers data-vm2_b (test)

**Endpoints disponibles** :
- `GET /db` - Retourne des donn√©es simul√©es
- `GET /whoami` - Informations sur l'instance
- `GET /health` - Health check

#### 4Ô∏è‚É£ **VM Admin** (Bonus)

**R√¥le** : Console d'administration pour surveiller toutes les VMs

- `admin-vm` (10.3.0.10:7000)
- Interroge tous les services et affiche leur statut
- Endpoint : `GET /probe` - Teste tous les serveurs

### Sch√©ma r√©seau complet

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      INTERNET                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                    [Public IP]
                         ‚îÇ
                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                 ‚îÇ   front-lb     ‚îÇ (Load Balancer Public)
                 ‚îÇ  - Port 80     ‚îÇ
                 ‚îÇ  - Port 8443   ‚îÇ
                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚îÇ Round-Robin
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                 ‚îÇ                 ‚îÇ
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇfrontend-vm1‚îÇ   ‚îÇfrontend-vm2‚îÇ   ‚îÇfrontend-vm2_b‚îÇ
  ‚îÇ 10.1.0.4  ‚îÇ   ‚îÇ 10.1.0.5   ‚îÇ   ‚îÇ 10.1.0.21 ‚îÇ
  ‚îÇ  Port 80  ‚îÇ   ‚îÇ Port 8443  ‚îÇ   ‚îÇ Port 8443 ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                ‚îÇ                 ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ VNet Peering
                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                 ‚îÇ    app-lb      ‚îÇ (Load Balancer Internal)
                 ‚îÇ  10.2.0.250    ‚îÇ
                 ‚îÇ  - Port 5000   ‚îÇ
                 ‚îÇ  - Port 5001   ‚îÇ
                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚îÇ Round-Robin
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                 ‚îÇ                 ‚îÇ
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ  app-vm1  ‚îÇ   ‚îÇ  app-vm2    ‚îÇ   ‚îÇ app-vm2_b ‚îÇ
  ‚îÇ 10.2.0.4  ‚îÇ   ‚îÇ 10.2.0.5    ‚îÇ   ‚îÇ 10.2.0.21 ‚îÇ
  ‚îÇ Port 5000 ‚îÇ   ‚îÇ Port 5001   ‚îÇ   ‚îÇ Port 5002 ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                ‚îÇ                 ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ VNet Peering
                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                 ‚îÇ    data-lb     ‚îÇ (Load Balancer Internal)
                 ‚îÇ  10.3.0.250    ‚îÇ
                 ‚îÇ  - Port 6000   ‚îÇ
                 ‚îÇ  - Port 6001   ‚îÇ
                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚îÇ Round-Robin
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                 ‚îÇ                 ‚îÇ
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ data-vm1  ‚îÇ   ‚îÇ data-vm2    ‚îÇ   ‚îÇdata-vm2_b ‚îÇ
  ‚îÇ 10.3.0.4  ‚îÇ   ‚îÇ 10.3.0.5    ‚îÇ   ‚îÇ 10.3.0.21 ‚îÇ
  ‚îÇ Port 6000 ‚îÇ   ‚îÇ Port 6001   ‚îÇ   ‚îÇ Port 6002 ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üõ†Ô∏è Pr√©requis

### Compte Azure

- Un abonnement Azure actif (gratuit ou payant)
- Azure CLI install√© : https://learn.microsoft.com/cli/azure/install-azure-cli

### Connaissances de base

- Commandes Linux de base (ssh, curl, grep)
- Concepts r√©seau (IP, port, subnet)
- Notions JavaScript/Node.js (optionnel)

### Outils locaux

```bash
# Installer Azure CLI sur Ubuntu/Debian
curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash

# Se connecter √† Azure
az login

# Cr√©er un resource group
az group create --name rg-loadbalancer --location francecentral
```

---

## üìÅ Structure des fichiers

```
azure-LB02/
‚îÇ
‚îú‚îÄ‚îÄ infra.sh                    # ‚≠ê Script principal de d√©ploiement Azure
‚îÇ
‚îú‚îÄ‚îÄ frontend/                   # Couche Frontend (port 80 et 8443)
‚îÇ   ‚îú‚îÄ‚îÄ frontend1.js            # Serveur Node.js pour frontend-vm1 (port 80)
‚îÇ   ‚îú‚îÄ‚îÄ frontend2.js            # Serveur Node.js pour frontend-vm2 (port 8443)
‚îÇ   ‚îú‚îÄ‚îÄ frontend2_b.js          # Serveur Node.js pour frontend-vm2_b (port 8443)
‚îÇ   ‚îú‚îÄ‚îÄ index1.html             # Page HTML pour frontend-vm1
‚îÇ   ‚îú‚îÄ‚îÄ index2.html             # Page HTML moderne pour frontend-vm2
‚îÇ   ‚îú‚îÄ‚îÄ index2_b.html           # Page HTML pour frontend-vm2_b
‚îÇ   ‚îú‚îÄ‚îÄ cloud-init-frontend1.yaml     # D√©ploiement automatique vm1
‚îÇ   ‚îú‚îÄ‚îÄ cloud-init-frontend2.yaml     # D√©ploiement automatique vm2
‚îÇ   ‚îî‚îÄ‚îÄ cloud-init-frontend2_b.yaml   # D√©ploiement automatique vm2_b
‚îÇ
‚îú‚îÄ‚îÄ app/                        # Couche Application (ports 5000, 5001, 5002)
‚îÇ   ‚îú‚îÄ‚îÄ app1.js                 # Serveur Node.js pour app-vm1
‚îÇ   ‚îú‚îÄ‚îÄ app2.js                 # Serveur Node.js pour app-vm2
‚îÇ   ‚îú‚îÄ‚îÄ app2_b.js               # Serveur Node.js pour app-vm2_b
‚îÇ   ‚îú‚îÄ‚îÄ cloud-init-app1.yaml
‚îÇ   ‚îú‚îÄ‚îÄ cloud-init-app2.yaml
‚îÇ   ‚îî‚îÄ‚îÄ cloud-init-app2_b.yaml
‚îÇ
‚îú‚îÄ‚îÄ data/                       # Couche Data (ports 6000, 6001, 6002)
‚îÇ   ‚îú‚îÄ‚îÄ data1.js                # Serveur Node.js pour data-vm1
‚îÇ   ‚îú‚îÄ‚îÄ data2.js                # Serveur Node.js pour data-vm2
‚îÇ   ‚îú‚îÄ‚îÄ data2_b.js              # Serveur Node.js pour data-vm2_b
‚îÇ   ‚îú‚îÄ‚îÄ cloud-init-data1.yaml
‚îÇ   ‚îú‚îÄ‚îÄ cloud-init-data2.yaml
‚îÇ   ‚îî‚îÄ‚îÄ cloud-init-data2_b.yaml
‚îÇ
‚îú‚îÄ‚îÄ admin/                      # VM Admin (monitoring)
‚îÇ   ‚îú‚îÄ‚îÄ admin.js                # Serveur Node.js de monitoring
‚îÇ   ‚îú‚îÄ‚îÄ index.html              # Interface admin
‚îÇ   ‚îî‚îÄ‚îÄ cloud-init-admin.yaml
‚îÇ
‚îú‚îÄ‚îÄ README.md                   # üìñ Ce fichier (documentation principale)
‚îú‚îÄ‚îÄ STACK-8443-README.md        # Documentation d√©taill√©e stack 8443
‚îú‚îÄ‚îÄ TEST-LB-APP-DATA.md         # Guide de test pour VMs _b
‚îî‚îÄ‚îÄ FRONTEND-2B-README.md       # Guide sp√©cifique frontend-vm2_b
```

### R√¥le de chaque fichier

#### **infra.sh** ‚≠ê
Le script Bash qui cr√©e **toute l'infrastructure Azure** :
- VNets et subnets
- Network Security Groups (NSG)
- Load Balancers (public et internes)
- Health Probes
- NAT Gateways
- NICs et VMs
- Bastion pour la connexion SSH

#### **Fichiers .js**
Serveurs Node.js/Express qui exposent des endpoints HTTP :
- √âcoutent sur un port sp√©cifique
- R√©pondent √† `/whoami`, `/health`, `/api`, `/db`
- Communiquent avec la couche suivante via Load Balancer

#### **Fichiers cloud-init .yaml**
Scripts d'initialisation automatique des VMs :
- Installent Node.js, npm, git
- Clonent ce d√©p√¥t GitHub
- Copient les fichiers .js
- Installent les d√©pendances (Express)
- D√©marrent le serveur automatiquement

#### **Fichiers .html**
Interfaces web statiques pour visualiser l'architecture et tester les endpoints

---

## üß† Concepts Azure expliqu√©s

### 1. Azure Load Balancer

**Qu'est-ce que c'est ?**
Un service Azure qui distribue le trafic r√©seau entre plusieurs serveurs (VMs). Il agit comme un "r√©partiteur" intelligent.

**Types de Load Balancer dans ce projet** :

#### **Public Load Balancer** (`front-lb`)
- Poss√®de une **IP publique** accessible depuis Internet
- Re√ßoit les requ√™tes HTTP/HTTPS des utilisateurs
- Distribue vers les VMs frontend dans le backend pool
- **Cas d'usage** : Site web, API publique

#### **Internal Load Balancer** (`app-lb` et `data-lb`)
- Poss√®de une **IP priv√©e** (10.2.0.250, 10.3.0.250)
- Accessible uniquement depuis les VNets Azure (via peering)
- Distribue le trafic entre les VMs internes
- **Cas d'usage** : Micro-services, bases de donn√©es

### 2. Backend Pool (Pool de backends)

**D√©finition** : Groupe de VMs qui re√ßoivent le trafic du Load Balancer

**Dans ce projet** :
- `front-backpool` : frontend-vm1, frontend-vm2, frontend-vm2_b
- `app-backpool` : app-vm1, app-vm2, app-vm2_b
- `data-backpool` : data-vm1, data-vm2, data-vm2_b

**Fonctionnement** : Le Load Balancer distribue les requ√™tes en **round-robin** :
```
Requ√™te 1 ‚Üí VM1
Requ√™te 2 ‚Üí VM2
Requ√™te 3 ‚Üí VM3
Requ√™te 4 ‚Üí VM1 (on recommence)
```

### 3. Health Probe (Sonde de sant√©)

**R√¥le** : V√©rifier qu'une VM est "en bonne sant√©" avant d'envoyer du trafic

**Fonctionnement** :
- Le Load Balancer envoie une requ√™te HTTP √† `/health` toutes les 5 secondes
- Si la VM r√©pond "OK", elle est marqu√©e **healthy** (saine)
- Si la VM ne r√©pond pas 2 fois cons√©cutives, elle est marqu√©e **unhealthy** (d√©faillante)
- Le Load Balancer n'envoie plus de trafic vers les VMs unhealthy

**Exemple dans le projet** :
```bash
# Health probe pour app-lb sur port 5000
az network lb probe create \
  --lb-name app-lb \
  --name ProbeApp1 \
  --protocol http \
  --path /health \
  --port 5000
```

**Code dans app1.js** :
```javascript
app.get('/health', (_, res) => res.send('OK'));
```

### 4. VNet Peering (Peering de r√©seaux virtuels)

**Probl√®me** : Les 3 couches sont dans des VNets diff√©rents (front-vnet, app-vnet, data-vnet)

**Solution** : Le **VNet Peering** permet la communication directe entre VNets

**Configuration dans infra.sh** :
```bash
# Peering front-vnet ‚Üî app-vnet
az network vnet peering create --vnet-name front-vnet --remote-vnet app-vnet
az network vnet peering create --vnet-name app-vnet --remote-vnet front-vnet

# Peering app-vnet ‚Üî data-vnet
az network vnet peering create --vnet-name app-vnet --remote-vnet data-vnet
az network vnet peering create --vnet-name data-vnet --remote-vnet app-vnet
```

**R√©sultat** : frontend-vm2 (10.1.0.5) peut communiquer avec app-lb (10.2.0.250)

### 5. Network Security Group (NSG)

**D√©finition** : Pare-feu virtuel qui contr√¥le le trafic entrant/sortant

**Dans ce projet** : Les NSG autorisent **tout le trafic** pour simplifier l'apprentissage
```bash
az network nsg rule create \
  --nsg-name front-nsg \
  --name AllowAll \
  --priority 100 \
  --source-address-prefixes '*' \
  --destination-port-ranges "*" \
  --access Allow
```

> ‚ö†Ô∏è **En production** : Il faut restreindre les ports (80, 443, 5000, 6000 uniquement)

### 6. NAT Gateway

**Probl√®me** : Les VMs internes (app-vnet, data-vnet) n'ont pas d'IP publique

**Solution** : Le **NAT Gateway** permet aux VMs internes de se connecter √† Internet (pour t√©l√©charger des paquets npm)

**Dans infra.sh** :
```bash
# Cr√©er NAT Gateway pour app-vnet
az network nat gateway create --name app-nat --public-ip-addresses nat-gateway-ip-app

# L'associer au subnet
az network vnet subnet update --vnet-name app-vnet --nat-gateway app-nat
```

### 7. Azure Bastion

**Probl√®me** : Comment se connecter en SSH aux VMs sans IP publique ?

**Solution** : **Azure Bastion** est un service de connexion s√©curis√©e sans exposer les VMs √† Internet

**Connexion** :
```bash
az network bastion ssh \
  --name bastion \
  --resource-group rg-loadbalancer \
  --auth-type password \
  --username cloud \
  --target-resource-id $(az vm show --name app-vm1 -g rg-loadbalancer --query id -o tsv)
```

### 8. Cloud-Init

**D√©finition** : Script d'initialisation qui s'ex√©cute automatiquement au d√©marrage d'une VM Ubuntu

**Exemple** (cloud-init-app1.yaml) :
```yaml
#cloud-config
package_update: true
packages:
  - nodejs
  - npm
  - git
runcmd:
  - git clone https://github.com/CallmeVRM/azure-LB02 /tmp/lab
  - cp /tmp/lab/app/app1.js /home/cloud/server.js
  - cd /home/cloud
  - npm init -y
  - npm install express
  - node /home/cloud/server.js &
```

**Avantage** : D√©ploiement 100% automatique, pas besoin de se connecter en SSH

---

## üöÄ D√©ploiement pas √† pas

### √âtape 1 : Cloner le projet

```bash
# Cloner le d√©p√¥t GitHub
git clone https://github.com/CallmeVRM/azure-LB02.git
cd azure-LB02
```

### √âtape 2 : Se connecter √† Azure

```bash
# Connexion interactive
az login

# V√©rifier l'abonnement actif
az account show --output table

# Cr√©er un resource group (groupe de ressources)
az group create --name rg-loadbalancer --location francecentral
```

### √âtape 3 : Ex√©cuter le script de d√©ploiement

```bash
# Rendre le script ex√©cutable
chmod +x infra.sh

# Lancer le d√©ploiement (dur√©e : 15-20 minutes)
./infra.sh
```

**Ce que fait le script** :
1. ‚úÖ Cr√©e 3 VNets (front, app, data)
2. ‚úÖ Configure le peering entre les VNets
3. ‚úÖ Cr√©e les NSG avec r√®gles de s√©curit√©
4. ‚úÖ Cr√©e un Azure Bastion
5. ‚úÖ Cr√©e 3 Load Balancers (1 public, 2 internes)
6. ‚úÖ Configure les health probes
7. ‚úÖ Cr√©e les NAT Gateways
8. ‚úÖ Cr√©e les NICs avec IPs priv√©es statiques
9. ‚úÖ Associe les NICs aux backend pools
10. ‚úÖ Cr√©e 9 VMs avec cloud-init (d√©ploiement automatique)

### √âtape 4 : V√©rifier le d√©ploiement

```bash
# Lister toutes les VMs cr√©√©es
az vm list --resource-group rg-loadbalancer --output table

# V√©rifier l'IP publique du Load Balancer frontend
az network public-ip show \
  --resource-group rg-loadbalancer \
  --name lb-pub-ip-in \
  --query ipAddress \
  --output tsv
```

### √âtape 5 : Attendre que les VMs soient pr√™tes

Cloud-init prend **2-3 minutes** pour installer Node.js et d√©marrer les serveurs.

```bash
# Se connecter √† une VM pour v√©rifier les logs cloud-init
az network bastion ssh \
  --name bastion \
  --resource-group rg-loadbalancer \
  --auth-type password \
  --username cloud \
  --target-resource-id $(az vm show --name app-vm1 -g rg-loadbalancer --query id -o tsv)

# Une fois connect√©, v√©rifier les logs
sudo tail -f /var/log/cloud-init-output.log

# V√©rifier que le serveur Node.js tourne
ps aux | grep node

# Tester localement
curl http://localhost:5000/health
# Devrait retourner : OK
```

---

## ‚öôÔ∏è Fonctionnement de l'application

### Flux d'une requ√™te compl√®te

Voici ce qui se passe quand un utilisateur visite `http://<PUBLIC_IP>:8443/api` :

```
1. Navigateur ‚Üí Internet ‚Üí Load Balancer Public (front-lb)
   [Requ√™te HTTP vers PUBLIC_IP:8443]

2. front-lb ‚Üí Choix d'une VM frontend (round-robin)
   [Distribue vers 10.1.0.5:8443 ou 10.1.0.21:8443]

3. frontend-vm2 re√ßoit la requ√™te sur /api
   Code : http.get('http://10.2.0.250:5001/api')
   [Appelle le Load Balancer app-lb]

4. app-lb ‚Üí Choix d'une VM app (round-robin)
   [Distribue vers 10.2.0.5:5001 ou 10.2.0.21:5002]

5. app-vm2 re√ßoit la requ√™te sur /api
   Code : http.get('http://10.3.0.250:6001/db')
   [Appelle le Load Balancer data-lb]

6. data-lb ‚Üí Choix d'une VM data (round-robin)
   [Distribue vers 10.3.0.5:6001 ou 10.3.0.21:6002]

7. data-vm2 re√ßoit la requ√™te sur /db
   Code : res.send('DATA-LAYER-2: OK')
   [Retourne une r√©ponse JSON]

8. R√©ponse remonte la cha√Æne
   data-vm2 ‚Üí app-vm2 ‚Üí frontend-vm2 ‚Üí front-lb ‚Üí Navigateur
```

### Endpoints d√©taill√©s par couche

#### **Frontend** (Ports 80, 8443)

| Endpoint | M√©thode | Description | Exemple de r√©ponse |
|----------|---------|-------------|-------------------|
| `/` | GET | Page HTML d'accueil | Interface web |
| `/whoami` | GET | Infos de l'instance frontend | `{ instance: "frontend-2", address: "10.1.0.5", port: 8443 }` |
| `/health` | GET | Health check | `OK` |
| `/api` | GET | Traverse toute la stack (Frontend‚ÜíApp‚ÜíData) | Donn√©es de la couche Data |
| `/probe/app` | GET | Interroge l'App Layer via LB | `{ instance: "app-1", ... }` |
| `/probe/data` | GET | Interroge la Data Layer via LB | `{ instance: "data-2", ... }` |

#### **Application** (Ports 5000, 5001, 5002)

| Endpoint | M√©thode | Description | Exemple de r√©ponse |
|----------|---------|-------------|-------------------|
| `/whoami` | GET | Infos de l'instance app | `{ instance: "app-1", address: "10.2.0.4", port: 5000 }` |
| `/health` | GET | Health check | `OK` |
| `/api` | GET | Appelle la couche Data | Donn√©es de la couche Data |

#### **Data** (Ports 6000, 6001, 6002)

| Endpoint | M√©thode | Description | Exemple de r√©ponse |
|----------|---------|-------------|-------------------|
| `/db` | GET | Retourne des donn√©es simul√©es | `DATA-LAYER-1: OK` |
| `/whoami` | GET | Infos de l'instance data | `{ instance: "data-1", address: "10.3.0.4", port: 6000 }` |
| `/health` | GET | Health check | `OK` |

#### **Admin** (Port 7000)

| Endpoint | M√©thode | Description | Exemple de r√©ponse |
|----------|---------|-------------|-------------------|
| `/` | GET | Interface admin | Dashboard HTML |
| `/status` | GET | Inventaire complet | `{ frontend: [...], app: [...], data: [...] }` |
| `/probe` | GET | Teste tous les serveurs | Statut de chaque VM |

### Code source expliqu√©

#### **frontend1.js** (frontend-vm1, port 80)

```javascript
const express = require('express');
const http = require('http');
const app = express();

// Configuration
const APP_LAYER = 'http://10.2.0.250:5000'; // IP du Load Balancer app-lb
const PORT = 80;

// Servir la page HTML
app.get('/', (req, res) => res.sendFile(path.join(__dirname, 'index1.html')));

// Endpoint pour identifier la VM
app.get('/whoami', (req, res) => {
  res.json({ 
    instance: 'frontend-1',           // Nom de l'instance
    address: req.socket.localAddress, // IP priv√©e
    port: PORT 
  });
});

// Proxy vers la couche App (traverse toute la stack)
app.get('/api', (_, res) => {
  http.get(APP_LAYER + '/api', r => r.pipe(res))  // Appelle app-lb
    .on('error', () => res.status(502).send('Bad Gateway'));
});

// Health check pour le Load Balancer
app.get('/health', (_, res) => res.send('OK'));

// D√©marrage du serveur
app.listen(PORT, '10.1.0.4', () => {
  console.log(`Frontend-1 listening on http://10.1.0.4:${PORT}`);
});
```

**Points cl√©s** :
- `APP_LAYER = 'http://10.2.0.250:5000'` : Appelle le **Load Balancer app-lb** (pas directement une VM)
- `app.listen(PORT, '10.1.0.4')` : √âcoute sur l'IP priv√©e statique de la VM
- `/api` : Utilise `http.get()` pour appeler la couche suivante

#### **app1.js** (app-vm1, port 5000)

```javascript
const express = require('express');
const http = require('http');
const app = express();

// Configuration
const DATA_LAYER = 'http://10.3.0.250:6000'; // IP du Load Balancer data-lb
const PORT = 5000;

// Endpoint qui appelle la couche Data
app.get('/api', (_, res) => {
  http.get(DATA_LAYER + '/db', r => r.pipe(res))  // Appelle data-lb
    .on('error', () => res.status(502).send('Bad Gateway'));
});

// Identification
app.get('/whoami', (req, res) => {
  res.json({ instance: 'app-1', address: req.socket.localAddress, port: PORT });
});

// Health check
app.get('/health', (_, res) => res.send('OK'));

// √âcoute sur toutes les interfaces (0.0.0.0 pour compatibilit√©)
app.listen(PORT, '0.0.0.0', () => {
  console.log(`APP1 listening on http://0.0.0.0:${PORT}`);
});
```

**Points cl√©s** :
- `DATA_LAYER = 'http://10.3.0.250:6000'` : Appelle le **Load Balancer data-lb**
- `app.listen(PORT, '0.0.0.0')` : √âcoute sur toutes les interfaces r√©seau

#### **data1.js** (data-vm1, port 6000)

```javascript
const express = require('express');
const app = express();
const PORT = 6000;

// Endpoint principal : simule une base de donn√©es
app.get('/db', (_, res) => {
  res.send('DATA-LAYER-1: OK');  // R√©ponse simple
});

// Identification
app.get('/whoami', (req, res) => {
  res.json({ instance: 'data-1', address: req.socket.localAddress, port: PORT });
});

// Health check
app.get('/health', (_, res) => res.send('OK'));

// D√©marrage
app.listen(PORT, '0.0.0.0', () => {
  console.log(`DATA1 listening on http://0.0.0.0:${PORT}`);
});
```

**Points cl√©s** :
- `/db` : Endpoint final de la cha√Æne, retourne une r√©ponse directe
- Pas d'appel HTTP vers d'autres services

---

## üß™ Tests et validation

### 1. Tester le Load Balancer frontend (depuis Internet)

```bash
# R√©cup√©rer l'IP publique
PUBLIC_IP=$(az network public-ip show \
  --resource-group rg-loadbalancer \
  --name lb-pub-ip-in \
  --query ipAddress \
  --output tsv)

echo "IP publique : $PUBLIC_IP"

# Tester le port 80 (frontend-vm1)
curl http://$PUBLIC_IP:80/whoami

# Tester le port 8443 (frontend-vm2 et frontend-vm2_b, round-robin)
for i in {1..10}; do
  curl -s http://$PUBLIC_IP:8443/whoami | grep -o '"instance":"[^"]*"'
done
# Devrait alterner entre frontend-2 et frontend-2_b
```

### 2. Tester la cha√Æne compl√®te (Frontend ‚Üí App ‚Üí Data)

```bash
# Appeler /api depuis le frontend (traverse toute la stack)
curl http://$PUBLIC_IP:8443/api

# R√©p√©ter pour voir la distribution round-robin
for i in {1..10}; do
  echo "=== Requ√™te $i ==="
  curl -s http://$PUBLIC_IP:8443/api
  sleep 1
done
```

### 3. Tester les Load Balancers internes (depuis une VM)

Les Load Balancers internes (app-lb, data-lb) ne sont pas accessibles depuis Internet. Il faut se connecter √† une VM pour les tester.

```bash
# Se connecter √† frontend-vm1 via Bastion
az network bastion ssh \
  --name bastion \
  --resource-group rg-loadbalancer \
  --auth-type password \
  --username cloud \
  --target-resource-id $(az vm show --name frontend-vm1 -g rg-loadbalancer --query id -o tsv)

# Une fois connect√©, tester app-lb
curl http://10.2.0.250:5000/whoami
curl http://10.2.0.250:5001/whoami

# Tester la distribution round-robin sur app-lb
for i in {1..10}; do
  curl -s http://10.2.0.250:5000/whoami | grep -o '"instance":"[^"]*"'
done
# Devrait alterner entre app-1 et app-2

# Tester data-lb
curl http://10.3.0.250:6000/whoami
curl http://10.3.0.250:6001/whoami

# Tester la distribution round-robin sur data-lb
for i in {1..10}; do
  curl -s http://10.3.0.250:6000/db
done
# Devrait alterner entre DATA-LAYER-1 et DATA-LAYER-2
```

### 4. Tester les health checks

```bash
# V√©rifier que toutes les VMs r√©pondent aux health checks
# Frontend
curl http://10.1.0.4:80/health        # frontend-vm1
curl http://10.1.0.5:8443/health      # frontend-vm2
curl http://10.1.0.21:8443/health     # frontend-vm2_b

# App
curl http://10.2.0.4:5000/health      # app-vm1
curl http://10.2.0.5:5001/health      # app-vm2
curl http://10.2.0.21:5002/health     # app-vm2_b

# Data
curl http://10.3.0.4:6000/health      # data-vm1
curl http://10.3.0.5:6001/health      # data-vm2
curl http://10.3.0.21:6002/health     # data-vm2_b

# Toutes devraient retourner "OK"
```

### 5. V√©rifier les backend pools sur Azure

```bash
# Voir les VMs dans le backend pool frontend
az network lb address-pool show \
  --resource-group rg-loadbalancer \
  --lb-name front-lb \
  --name front-backpool \
  --query backendIPConfigurations[].id -o table

# Voir les health probes
az network lb probe list \
  --resource-group rg-loadbalancer \
  --lb-name app-lb \
  --output table

# V√©rifier qu'une VM est healthy
az network nic show \
  --resource-group rg-loadbalancer \
  --name app-nic-vm1 \
  --query "ipConfigurations[0].loadBalancerBackendAddressPools" \
  --output json
```

### 6. Simuler une panne pour tester la haute disponibilit√©

```bash
# Se connecter √† app-vm1
az network bastion ssh \
  --name bastion \
  --resource-group rg-loadbalancer \
  --auth-type password \
  --username cloud \
  --target-resource-id $(az vm show --name app-vm1 -g rg-loadbalancer --query id -o tsv)

# Arr√™ter le serveur Node.js
pkill -f node

# Depuis une autre VM, tester app-lb
for i in {1..20}; do
  curl -s http://10.2.0.250:5000/whoami | grep -o '"instance":"[^"]*"'
done
# Ne devrait retourner QUE "app-2" car app-1 est down

# Red√©marrer le serveur sur app-vm1
cd /home/cloud
nohup node server.js > server.log 2>&1 &

# Attendre 10-15 secondes (temps de d√©tection du health probe)
# Retester : les deux instances devraient r√©appara√Ætre
```

### 7. Tester l'interface admin (monitoring)

```bash
# Se connecter √† admin-vm
az network bastion ssh \
  --name bastion \
  --resource-group rg-loadbalancer \
  --auth-type password \
  --username cloud \
  --target-resource-id $(az vm show --name admin-vm -g rg-loadbalancer --query id -o tsv)

# V√©rifier que le serveur admin tourne
curl http://localhost:7000/status

# Tester le probe complet (teste toutes les VMs)
curl http://10.3.0.10:7000/probe | jq .
```

**Exemple de sortie `/probe`** :
```json
{
  "frontend": [
    { "host": "10.1.0.4:80", "whoami": { "ok": true, "body": "{\"instance\":\"frontend-1\"}" }, "health": { "ok": true } },
    { "host": "10.1.0.5:8443", "whoami": { "ok": true, "body": "{\"instance\":\"frontend-2\"}" }, "health": { "ok": true } }
  ],
  "app": [
    { "host": "10.2.0.4:5000", "whoami": { "ok": true }, "health": { "ok": true } },
    { "host": "10.2.0.5:5001", "whoami": { "ok": true }, "health": { "ok": true } }
  ],
  "data": [
    { "host": "10.3.0.4:6000", "whoami": { "ok": true }, "health": { "ok": true } },
    { "host": "10.3.0.5:6001", "whoami": { "ok": true }, "health": { "ok": true } }
  ]
}
```

---

## üêõ Troubleshooting

### Probl√®me 1 : "Connection refused" lors du test d'un endpoint

**Sympt√¥mes** :
```bash
curl http://10.2.0.5:5001/health
curl: (7) Failed to connect to 10.2.0.5 port 5001: Connection refused
```

**Causes possibles** :
1. Le serveur Node.js n'est pas d√©marr√©
2. Cloud-init n'a pas encore termin√©
3. Le serveur √©coute sur la mauvaise IP

**Solutions** :
```bash
# Se connecter √† la VM
az network bastion ssh --name bastion --resource-group rg-loadbalancer \
  --auth-type password --username cloud \
  --target-resource-id $(az vm show --name app-vm2 -g rg-loadbalancer --query id -o tsv)

# V√©rifier les logs cloud-init
sudo tail -100 /var/log/cloud-init-output.log | grep -i error

# V√©rifier que Node.js est install√©
node --version
npm --version

# V√©rifier que le serveur tourne
ps aux | grep node

# Si pas de processus, v√©rifier le fichier
ls -la /home/cloud/server.js

# D√©marrer manuellement
cd /home/cloud
node server.js &

# Tester localement
curl http://localhost:5001/health
```

### Probl√®me 2 : Health probe √©choue sur Azure

**Sympt√¥mes** : Dans le portail Azure, la VM est marqu√©e "Unhealthy"

**Solutions** :
```bash
# V√©rifier la configuration du health probe
az network lb probe show \
  --resource-group rg-loadbalancer \
  --lb-name app-lb \
  --name ProbeApp2

# V√©rifier que l'endpoint /health r√©pond localement
ssh vers la VM
curl http://localhost:5001/health
# Doit retourner "OK"

# V√©rifier que le port est bien ouvert
sudo ss -tlnp | grep 5001

# V√©rifier les NSG (ne devraient pas bloquer)
az network nsg rule list \
  --resource-group rg-loadbalancer \
  --nsg-name app-nsg \
  --output table
```

### Probl√®me 3 : Round-robin ne fonctionne pas

**Sympt√¥mes** : Toutes les requ√™tes vont vers la m√™me VM

**Causes** :
- Une seule VM est healthy
- Session affinity activ√©e (pas le cas dans ce projet)

**Solutions** :
```bash
# V√©rifier que toutes les VMs sont healthy
for i in {1..10}; do
  curl -s http://10.2.0.250:5000/whoami
  sleep 0.5
done

# V√©rifier le backend pool
az network lb address-pool show \
  --resource-group rg-loadbalancer \
  --lb-name app-lb \
  --name app-backpool

# V√©rifier que les NICs sont bien dans le backend pool
az network nic show --name app-nic-vm1 -g rg-loadbalancer \
  --query "ipConfigurations[0].loadBalancerBackendAddressPools"
```

### Probl√®me 4 : "Cannot find module 'express'"

**Sympt√¥mes** :
```
Error: Cannot find module 'express'
```

**Solutions** :
```bash
# Se connecter √† la VM
# Installer express manuellement
cd /home/cloud
npm init -y
npm install express

# Red√©marrer le serveur
pkill -f node
nohup node server.js > server.log 2>&1 &
```

### Probl√®me 5 : Cloud-init n'a pas clon√© le d√©p√¥t GitHub

**Sympt√¥mes** :
```
cp: cannot stat '/tmp/lab/app/app1.js': No such file or directory
```

**Causes** : Probl√®me r√©seau, NAT Gateway pas encore pr√™t

**Solutions** :
```bash
# Cloner manuellement
git clone https://github.com/CallmeVRM/azure-LB02.git /tmp/lab

# Copier les fichiers
cp /tmp/lab/app/app1.js /home/cloud/server.js

# Installer les d√©pendances
cd /home/cloud
npm init -y
npm install express

# D√©marrer le serveur
node server.js &
```

### Probl√®me 6 : Impossible de se connecter via Bastion

**Sympt√¥mes** :
```
Target subscription/resource group/resources could not be found.
```

**Solutions** :
```bash
# V√©rifier que Bastion est d√©ploy√©
az network bastion show --name bastion --resource-group rg-loadbalancer

# V√©rifier que la VM existe
az vm show --name app-vm1 --resource-group rg-loadbalancer

# Alternative : utiliser serial console dans le portail Azure
# Portail ‚Üí VM ‚Üí Serial Console
```

### Probl√®me 7 : Timeout sur les requ√™tes entre couches

**Sympt√¥mes** : frontend ‚Üí app fonctionne, mais app ‚Üí data timeout

**Causes** : Peering VNet manquant, NSG trop restrictif

**Solutions** :
```bash
# V√©rifier le peering
az network vnet peering list --resource-group rg-loadbalancer --vnet-name app-vnet --output table

# Tester la connectivit√© depuis app-vm1
ssh vers app-vm1
ping 10.3.0.250  # IP de data-lb
curl http://10.3.0.250:6000/health

# V√©rifier les routes r√©seau
ip route show
```

---

## üéì Exercices pratiques

### Exercice 1 : Ajouter une nouvelle VM frontend

**Objectif** : Comprendre comment ajouter un backend √† un Load Balancer existant

**√âtapes** :
1. Cr√©er une nouvelle NIC avec IP statique 10.1.0.22
2. Cr√©er une nouvelle VM frontend-vm3
3. Ajouter la NIC au backend pool `front-backpool`
4. Tester la distribution sur 4 VMs

<details>
<summary>Solution (cliquer pour afficher)</summary>

```bash
# 1. Cr√©er la NIC
az network nic create -g rg-loadbalancer \
  -n front-nic-vm3 \
  --vnet-name front-vnet \
  --subnet vm-subnet \
  --private-ip-address 10.1.0.22

# 2. Ajouter au backend pool
az network nic ip-config address-pool add \
  -g rg-loadbalancer \
  --lb-name front-lb \
  --address-pool front-backpool \
  --nic-name front-nic-vm3 \
  --ip-config-name ipconfig1

# 3. Cr√©er la VM (copier frontend1.js et modifier instance: 'frontend-3')
az vm create -g rg-loadbalancer \
  -n frontend-vm3 \
  --nics front-nic-vm3 \
  --image Ubuntu2404 \
  --admin-username cloud \
  --admin-password 'Motdepassefort123!' \
  --custom-data @frontend/cloud-init-frontend1.yaml \
  --size Standard_B1s

# 4. Tester
for i in {1..20}; do curl -s http://$PUBLIC_IP:80/whoami; done
```
</details>

### Exercice 2 : Modifier le code pour afficher un message personnalis√©

**Objectif** : Comprendre comment modifier et red√©ployer le code

**√âtapes** :
1. Modifier `data1.js` pour retourner `{ message: "Hello from Data Layer!", instance: "data-1" }`
2. Red√©ployer sur data-vm1
3. Tester l'endpoint `/db`

<details>
<summary>Solution</summary>

```bash
# 1. Modifier localement
nano data/data1.js
# Changer :
# app.get('/db', (_, res) => res.json({ message: "Hello from Data Layer!", instance: "data-1" }));

# 2. Commiter et pusher
git add data/data1.js
git commit -m "Update data1.js message"
git push

# 3. Se connecter √† data-vm1
az network bastion ssh --name bastion -g rg-loadbalancer \
  --auth-type password --username cloud \
  --target-resource-id $(az vm show --name data-vm1 -g rg-loadbalancer --query id -o tsv)

# 4. Mettre √† jour le code
cd /tmp
git clone https://github.com/CallmeVRM/azure-LB02.git
cp azure-LB02/data/data1.js /home/cloud/server.js

# 5. Red√©marrer
pkill -f node
cd /home/cloud
nohup node server.js > server.log 2>&1 &

# 6. Tester
curl http://localhost:6000/db
```
</details>

### Exercice 3 : Configurer un nouveau port sur le Load Balancer

**Objectif** : Ajouter un nouveau health probe et une r√®gle de load balancing

**√âtapes** :
1. Ajouter un health probe sur port 5002 pour app-lb
2. Cr√©er une r√®gle de load balancing pour port 5002
3. V√©rifier que app-vm2_b re√ßoit du trafic sur ce port

<details>
<summary>Solution</summary>

```bash
# 1. Cr√©er le health probe
az network lb probe create \
  -g rg-loadbalancer \
  --lb-name app-lb \
  --name ProbeApp3 \
  --protocol http \
  --path /health \
  --port 5002

# 2. Cr√©er la r√®gle
az network lb rule create \
  -g rg-loadbalancer \
  --lb-name app-lb \
  --name App5002 \
  --protocol TCP \
  --frontend-port 5002 \
  --backend-port 5002 \
  --frontend-ip-name app-front-ip \
  --backend-pool-name app-backpool \
  --probe-name ProbeApp3

# 3. Tester
ssh vers frontend-vm1
curl http://10.2.0.250:5002/whoami
# Devrait retourner app-2_b
```
</details>

### Exercice 4 : Surveiller les m√©triques Azure

**Objectif** : Utiliser Azure Monitor pour voir les m√©triques du Load Balancer

**√âtapes** :
1. Aller dans le portail Azure ‚Üí Load Balancer ‚Üí Metrics
2. Afficher le graphique "Data Path Availability" (disponibilit√©)
3. Afficher "Health Probe Status" (statut des sondes)
4. Simuler une panne et observer les m√©triques

### Exercice 5 : Cr√©er un endpoint de stress test

**Objectif** : Comprendre comment le Load Balancer g√®re la charge

**√âtapes** :
1. Ajouter un endpoint `/heavy` dans `app1.js` qui simule un traitement lourd
2. Utiliser Apache Bench pour envoyer 1000 requ√™tes
3. Observer la distribution entre app-vm1 et app-vm2

<details>
<summary>Solution</summary>

```javascript
// Dans app1.js, ajouter :
app.get('/heavy', (req, res) => {
  // Simuler un traitement lourd (calculs intensifs)
  let sum = 0;
  for (let i = 0; i < 10000000; i++) {
    sum += Math.sqrt(i);
  }
  res.json({ 
    instance: 'app-1', 
    result: sum,
    timestamp: new Date().toISOString()
  });
});
```

```bash
# Installer Apache Bench
sudo apt-get install apache2-utils

# Lancer le test (1000 requ√™tes, 50 concurrentes)
ab -n 1000 -c 50 http://10.2.0.250:5000/heavy

# Observer les r√©sultats
```
</details>

---

## üìö Ressources compl√©mentaires

### Documentation officielle Azure

- [Azure Load Balancer - Vue d'ensemble](https://learn.microsoft.com/azure/load-balancer/load-balancer-overview)
- [VNet Peering](https://learn.microsoft.com/azure/virtual-network/virtual-network-peering-overview)
- [Azure Bastion](https://learn.microsoft.com/azure/bastion/bastion-overview)
- [Cloud-Init sur Azure](https://learn.microsoft.com/azure/virtual-machines/linux/using-cloud-init)
- [NSG (Network Security Groups)](https://learn.microsoft.com/azure/virtual-network/network-security-groups-overview)

### Tutoriels recommand√©s

- [Microsoft Learn - Load Balancer](https://learn.microsoft.com/training/modules/improve-app-scalability-resiliency-with-load-balancer/)
- [Azure Architecture Center - Load Balancing](https://learn.microsoft.com/azure/architecture/guide/technology-choices/load-balancing-overview)

### Outils utiles

- **Azure CLI** : https://learn.microsoft.com/cli/azure/
- **Azure Portal** : https://portal.azure.com
- **VS Code + Azure Extension** : Pour √©diter les fichiers et d√©ployer
- **Postman** : Pour tester les APIs

---

## üßπ Nettoyage (Supprimer toutes les ressources)

**Important** : Les ressources Azure co√ªtent de l'argent. N'oubliez pas de les supprimer apr√®s vos tests !

```bash
# Supprimer TOUT le resource group (supprime toutes les ressources d'un coup)
az group delete --name rg-loadbalancer --yes --no-wait

# V√©rifier que la suppression est en cours
az group show --name rg-loadbalancer --query properties.provisioningState

# Lister tous les resource groups
az group list --output table
```

**Alternative** : Supprimer uniquement les VMs pour √©conomiser de l'argent, mais garder l'infra r√©seau

```bash
# Lister toutes les VMs
az vm list -g rg-loadbalancer --query "[].name" -o tsv

# Supprimer toutes les VMs
for vm in $(az vm list -g rg-loadbalancer --query "[].name" -o tsv); do
  az vm delete -g rg-loadbalancer -n $vm --yes --no-wait
done

# Les Load Balancers, VNets, NSG restent et ne co√ªtent presque rien
```

---

## üìù Checklist de validation finale

Avant de consid√©rer le lab comme termin√©, v√©rifiez :

- [ ] Toutes les 9 VMs sont d√©ploy√©es et running
- [ ] Les 3 Load Balancers (front-lb, app-lb, data-lb) fonctionnent
- [ ] Les health probes sont tous "healthy" dans le portail Azure
- [ ] Le test `/whoami` retourne bien le nom de chaque instance
- [ ] Le round-robin fonctionne (alterner entre VMs)
- [ ] La cha√Æne compl√®te Frontend ‚Üí App ‚Üí Data fonctionne
- [ ] Azure Bastion permet de se connecter aux VMs
- [ ] Les logs cloud-init ne montrent pas d'erreurs
- [ ] L'interface admin (port 7000) fonctionne
- [ ] La simulation de panne (pkill node) d√©clenche le failover

---

## üéØ Objectifs p√©dagogiques atteints

Apr√®s avoir compl√©t√© ce lab, vous devriez √™tre capable de :

‚úÖ Expliquer le r√¥le d'un Load Balancer  
‚úÖ Cr√©er un Load Balancer Public et Internal dans Azure  
‚úÖ Configurer des Backend Pools avec plusieurs VMs  
‚úÖ Mettre en place des Health Probes  
‚úÖ Configurer le VNet Peering pour connecter des r√©seaux  
‚úÖ D√©ployer des VMs avec Cloud-Init  
‚úÖ D√©bugger des probl√®mes r√©seau dans Azure  
‚úÖ Utiliser Azure Bastion pour la connexion SSH  
‚úÖ Comprendre le round-robin et la haute disponibilit√©  
‚úÖ Tester et valider une architecture multi-tiers

---

## üí° Cas d'usage r√©els

Ce type d'architecture est utilis√© dans :

- **E-commerce** : R√©partir le trafic entre plusieurs serveurs web
- **APIs REST** : Distribuer les requ√™tes API sur plusieurs instances
- **Micro-services** : Isoler frontend, backend, base de donn√©es
- **High Availability** : Assurer la continuit√© m√™me si un serveur tombe
- **Scalabilit√©** : Ajouter/retirer des serveurs selon la charge

---

**Auteur** : VRM  
**Projet** : Azure Load Balancer Lab  
**Repository** : [github.com/CallmeVRM/azure-LB02](https://github.com/CallmeVRM/azure-LB02)  
**Licence** : MIT  
**Date** : 2025

---

## üìß Support

Pour toute question ou probl√®me :
- Ouvrir une **issue** sur GitHub
- Consulter les logs cloud-init : `sudo tail -f /var/log/cloud-init-output.log`
- V√©rifier la documentation Azure officielle

**Bon apprentissage ! üöÄ**
