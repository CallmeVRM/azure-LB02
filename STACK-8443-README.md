#  Documentation Compl√®te G√©n√©r√© par Claude via copilot, √©bauche √† corriger...

## üéØ Vue d'ensemble

Cette stack (port 8443) repr√©sente une architecture 3-tiers compl√®te avec load balancing Azure. Le projet inclut maintenant des **VMs suppl√©mentaires (_b)** pour tester la r√©partition de charge avec 3 backends au lieu de 2.

### Architecture de base (port 8443)

```
Frontend-2 / Frontend-2_b (port 8443)
    ‚Üì
App-LB (10.2.0.250:5001/5002)
    ‚Üì
App-2 / App-2_b (port 5001/5002)
    ‚Üì
Data-LB (10.3.0.250:6001/6002)
    ‚Üì
Data-2 / Data-2_b (port 6001/6002)
```

### Architecture compl√®te avec Load Balancers

```
                  Frontend Load Balancer (Public IP)
                            |
        +-------------------+-------------------+
        |                   |                   |
   frontend-vm1      frontend-vm2        frontend-vm2_b
   10.1.0.10:80      10.1.0.20:8443       10.1.0.21:8443
        |                   |                   |
        +-------------------+-------------------+
                            |
                   App Load Balancer (10.2.0.250)
                            |
        +-------------------+-------------------+
        |                   |                   |
     app-vm1            app-vm2            app-vm2_b
   10.2.0.10:5000     10.2.0.20:5001      10.2.0.21:5002
        |                   |                   |
        +-------------------+-------------------+
                            |
                   Data Load Balancer (10.3.0.250)
                            |
        +-------------------+-------------------+
        |                   |                   |
    data-vm1           data-vm2           data-vm2_b
  10.3.0.10:6000     10.3.0.20:6001     10.3.0.21:6002
```

## ‚ú® Fonctionnalit√©s


### Architecture :
- ‚úÖ **Timeouts** sur toutes les requ√™tes HTTP (5 secondes)
- ‚úÖ **Gestion d'erreurs** compl√®te avec logging
- ‚úÖ **M√©triques** disponibles sur chaque couche
- ‚úÖ **√âcoute sur 0.0.0.0** pour compatibilit√© maximale
- ‚úÖ **D√©tection automatique** des changements de backend via Load Balancer

## üì¶ Composants

### Couche Frontend

#### Frontend-2 (`frontend/frontend2.js`)
- **VM** : frontend-vm2
- **IP Priv√©e** : 10.1.0.20
- **Port** : 8443
- **Instance** : `frontend-2`
- **Cible App** : http://10.2.0.250:5001
- **Fichiers** : `frontend2.js`, `index2.html`, `cloud-init-frontend2.yaml`

#### Frontend-2_b (`frontend/frontend2_b.js`)
- **VM** : frontend-vm2_b
- **IP Priv√©e** : 10.1.0.21
- **Port** : 8443
- **Instance** : `frontend-2_b`
- **Cible App** : http://10.2.0.250:5001
- **Fichiers** : `frontend2_b.js`, `index2_b.html`, `cloud-init-frontend2_b.yaml`

**Endpoints frontends** :
- `GET /` - Interface utilisateur moderne avec cards et animations
- `GET /whoami` - Retourne `{ instance, address, port, timestamp }`
- `GET /health` - Retourne `OK` (pour health probes)
- `GET /api` - Proxy vers la couche App (traverse toute la stack)
- `GET /probe/app` - R√©cup√®re les infos de l'app via LB (JSON)
- `GET /probe/app-health` - Health check de l'app layer
- `GET /probe/data` - R√©cup√®re les infos de la data via LB (JSON)
- `GET /probe/data-health` - Health check de la data layer
- `GET /metrics` - M√©triques de monitoring (uptime, memory)

**Caract√©ristiques** :
- ‚úÖ √âcoute sur `0.0.0.0:8443`
- ‚úÖ Timeouts de 5 secondes sur toutes les requ√™tes HTTP
- ‚úÖ Gestion d'erreurs robuste avec logs structur√©s
- ‚úÖ Interface web avec auto-refresh toutes les 3 secondes
- ‚úÖ Animations visuelles lors des changements d'instance

### Couche Application

#### App-2 (`app/app2.js`)
- **VM** : app-vm2
- **IP Priv√©e** : 10.2.0.20
- **Port** : 5001
- **Instance** : `app-2`
- **Cible Data** : http://10.3.0.250:6001
- **Fichiers** : `app2.js`, `cloud-init-app2.yaml`

#### App-2_b (`app/app2_b.js`)
- **VM** : app-vm2_b
- **IP Priv√©e** : 10.2.0.21
- **Port** : 5002
- **Instance** : `app-2_b`
- **Cible Data** : http://10.3.0.250:6002
- **Fichiers** : `app2_b.js`, `cloud-init-app2_b.yaml`

**Endpoints apps** :
- `GET /whoami` - Retourne `{ instance, address, port, timestamp }`
- `GET /health` - Retourne `OK` (pour health probes)
- `GET /api` - Proxy vers la couche Data (appelle `/db`)
- `GET /metrics` - M√©triques de monitoring

**Caract√©ristiques** :
- ‚úÖ √âcoute sur `0.0.0.0`
- ‚úÖ Timeouts de 5 secondes sur les appels data
- ‚úÖ Gestion d'erreurs avec fallback JSON
- ‚úÖ Logging avec timestamps ISO

### Couche Data

#### Data-2 (`data/data2.js`)
- **VM** : data-vm2
- **IP Priv√©e** : 10.3.0.20
- **Port** : 6001
- **Instance** : `data-2`
- **Fichiers** : `data2.js`, `cloud-init-data2.yaml`

#### Data-2_b (`data/data2_b.js`)
- **VM** : data-vm2_b
- **IP Priv√©e** : 10.3.0.21
- **Port** : 6002
- **Instance** : `data-2_b`
- **Fichiers** : `data2_b.js`, `cloud-init-data2_b.yaml`

**Endpoints data** :
- `GET /db` - Retourne `{ message, instance, timestamp }` (donn√©es principales)
- `GET /whoami` - Retourne `{ instance, address, port, timestamp }`
- `GET /health` - Retourne `OK` (pour health probes)
- `GET /metrics` - M√©triques avec compteur de requ√™tes

**Caract√©ristiques** :
- ‚úÖ √âcoute sur `0.0.0.0`
- ‚úÖ Compteur de requ√™tes pour monitoring
- ‚úÖ Logging de toutes les requ√™tes
- ‚úÖ R√©ponses JSON structur√©es

## üîó Flux de donn√©es

### Appel complet (Frontend ‚Üí App ‚Üí Data)

```
1. Browser ‚Üí Frontend LB (Public IP:8443)
2. Frontend LB ‚Üí frontend-vm2 ou frontend-vm2_b (round-robin)
3. Frontend ‚Üí GET /api
4. Frontend ‚Üí App LB (10.2.0.250:5001 ou 5002)
5. App LB ‚Üí app-vm2 ou app-vm2_b (round-robin)
6. App ‚Üí GET /api ‚Üí proxy vers Data LB
7. App ‚Üí Data LB (10.3.0.250:6001 ou 6002)
8. Data LB ‚Üí data-vm2 ou data-vm2_b (round-robin)
9. Data ‚Üí GET /db ‚Üí retourne JSON
10. R√©ponse remonte la cha√Æne : Data ‚Üí App ‚Üí Frontend ‚Üí Browser
```

### Probes serveur-side (pour √©viter CORS)

```
Browser ‚Üí GET /probe/app
Frontend ‚Üí http://10.2.0.250:5001/whoami
App LB ‚Üí app-vm2 ou app-vm2_b
Response JSON ‚Üí Frontend ‚Üí Browser
```

## üöÄ D√©ploiement

### Vue d'ensemble

Ce projet utilise **cloud-init** pour automatiser le d√©ploiement. Chaque VM clone le d√©p√¥t GitHub, copie les fichiers n√©cessaires, installe les d√©pendances et d√©marre le serveur automatiquement.

> ‚ö†Ô∏è **Important** : Avant de d√©ployer les VMs, assurez-vous que tous les fichiers sont **commit√©s et push√©s** sur GitHub, car cloud-init clone le repo au d√©marrage.

### √âtape 1 : Commiter les fichiers sur GitHub

```bash
# Ajouter tous les nouveaux fichiers
git add frontend/frontend2*.js frontend/index2*.html frontend/cloud-init-frontend2*.yaml
git add app/app2*.js app/cloud-init-app2*.yaml
git add data/data2*.js data/cloud-init-data2*.yaml
git add STACK-8443-README.md TEST-LB-APP-DATA.md FRONTEND-2B-README.md

# Commiter
git commit -m "Add stack 8443 with test VMs (_b variants)"

# Pousser sur GitHub
git push origin main
```

### √âtape 2 : D√©ploiement automatique (Cloud-Init)

Les fichiers cloud-init sont d√©j√† configur√©s :
- VMs principales : `cloud-init-frontend2.yaml`, `cloud-init-app2.yaml`, `cloud-init-data2.yaml`
- VMs de test : `cloud-init-frontend2_b.yaml`, `cloud-init-app2_b.yaml`, `cloud-init-data2_b.yaml`

**Cr√©ation des VMs avec cloud-init** :

```bash
# Frontend-VM2 (port 8443)
az vm create \
  --resource-group rg-loadbalancer \
  --name frontend-vm2 \
  --vnet-name vnet-main \
  --subnet subnet-frontend \
  --private-ip-address 10.1.0.20 \
  --public-ip-address pip-frontend-vm2 \
  --nsg nsg-frontend \
  --image Ubuntu2204 \
  --size Standard_B1s \
  --custom-data frontend/cloud-init-frontend2.yaml

# Frontend-VM2_b (port 8443, VM de test)
az vm create \
  --resource-group rg-loadbalancer \
  --name frontend-vm2_b \
  --vnet-name vnet-main \
  --subnet subnet-frontend \
  --private-ip-address 10.1.0.21 \
  --public-ip-address pip-frontend-vm2-b \
  --nsg nsg-frontend \
  --image Ubuntu2204 \
  --size Standard_B1s \
  --custom-data frontend/cloud-init-frontend2_b.yaml

# App-VM2 (port 5001)
az vm create \
  --resource-group rg-loadbalancer \
  --name app-vm2 \
  --vnet-name vnet-main \
  --subnet subnet-app \
  --private-ip-address 10.2.0.20 \
  --nsg nsg-app \
  --image Ubuntu2204 \
  --size Standard_B1s \
  --custom-data app/cloud-init-app2.yaml

# App-VM2_b (port 5002, VM de test)
az vm create \
  --resource-group rg-loadbalancer \
  --name app-vm2_b \
  --vnet-name vnet-main \
  --subnet subnet-app \
  --private-ip-address 10.2.0.21 \
  --nsg nsg-app \
  --image Ubuntu2204 \
  --size Standard_B1s \
  --custom-data app/cloud-init-app2_b.yaml

# Data-VM2 (port 6001)
az vm create \
  --resource-group rg-loadbalancer \
  --name data-vm2 \
  --vnet-name vnet-main \
  --subnet subnet-data \
  --private-ip-address 10.3.0.20 \
  --nsg nsg-data \
  --image Ubuntu2204 \
  --size Standard_B1s \
  --custom-data data/cloud-init-data2.yaml

# Data-VM2_b (port 6002, VM de test)
az vm create \
  --resource-group rg-loadbalancer \
  --name data-vm2_b \
  --vnet-name vnet-main \
  --subnet subnet-data \
  --private-ip-address 10.3.0.21 \
  --nsg nsg-data \
  --image Ubuntu2204 \
  --size Standard_B1s \
  --custom-data data/cloud-init-data2_b.yaml
```

> üìò **Note pour les VMs `_b`** : Vous devez √©galement ajouter les r√®gles de load balancer et health probes dans `infra.sh` pour les ports **5002** et **6002**. Voir `TEST-LB-APP-DATA.md` pour les d√©tails.

### √âtape 3 : Ajouter les VMs aux backend pools

```bash
# Frontend VMs ‚Üí Frontend LB backend pool
az network nic ip-config address-pool add \
  --resource-group rg-loadbalancer \
  --nic-name frontend-vm2VMNic \
  --ip-config-name ipconfig1 \
  --lb-name lb-frontend \
  --address-pool bepool-frontend

az network nic ip-config address-pool add \
  --resource-group rg-loadbalancer \
  --nic-name frontend-vm2_bVMNic \
  --ip-config-name ipconfig1 \
  --lb-name lb-frontend \
  --address-pool bepool-frontend

# App VMs ‚Üí App LB backend pool
az network nic ip-config address-pool add \
  --resource-group rg-loadbalancer \
  --nic-name app-vm2VMNic \
  --ip-config-name ipconfig1 \
  --lb-name lb-app \
  --address-pool bepool-app

az network nic ip-config address-pool add \
  --resource-group rg-loadbalancer \
  --nic-name app-vm2_bVMNic \
  --ip-config-name ipconfig1 \
  --lb-name lb-app \
  --address-pool bepool-app

# Data VMs ‚Üí Data LB backend pool
az network nic ip-config address-pool add \
  --resource-group rg-loadbalancer \
  --nic-name data-vm2VMNic \
  --ip-config-name ipconfig1 \
  --lb-name lb-data \
  --address-pool bepool-data

az network nic ip-config address-pool add \
  --resource-group rg-loadbalancer \
  --nic-name data-vm2_bVMNic \
  --ip-config-name ipconfig1 \
  --lb-name lb-data \
  --address-pool bepool-data
```

### √âtape 4 : V√©rification du d√©ploiement

```bash
# V√©rifier que cloud-init a termin√© (peut prendre 2-3 minutes)
ssh cloud@<VM-IP> "sudo tail -f /var/log/cloud-init-output.log"

# V√©rifier que le serveur est en cours d'ex√©cution
ssh cloud@<VM-IP> "ps aux | grep node"

# Tester localement sur la VM
ssh cloud@<VM-IP> "curl http://localhost:<PORT>/health"
# Frontend: PORT=8443
# App-2: PORT=5001, App-2_b: PORT=5002
# Data-2: PORT=6001, Data-2_b: PORT=6002
```

### Option alternative : D√©ploiement manuel

Si vous devez d√©ployer manuellement (troubleshooting ou d√©veloppement local) :

#### Sur frontend-vm2
```bash
# Se connecter √† la VM
az network bastion ssh --name bastion --resource-group $rg \
  --auth-type password --username cloud \
  --target-resource-id $(az vm show --name frontend-vm2 -g $rg --query id -o tsv)

# Cloner le repo et installer
mkdir -p /home/cloud/frontend
cd /home/cloud/frontend
git clone https://github.com/CallmeVRM/azure-LB02 /tmp/lab
cp /tmp/lab/frontend/frontend2.js ./server.js
cp /tmp/lab/frontend/index2.html ./index2.html
npm init -y
npm install express

# D√©marrer le serveur
node server.js
```

#### Sur frontend-vm2_b
```bash
# M√™me proc√©dure que frontend-vm2, mais avec :
cp /tmp/lab/frontend/frontend2_b.js ./server.js
cp /tmp/lab/frontend/index2_b.html ./index2_b.html
```

#### Sur app-vm2
```bash
# Se connecter √† la VM
az network bastion ssh --name bastion --resource-group $rg \
  --auth-type password --username cloud \
  --target-resource-id $(az vm show --name app-vm2 -g $rg --query id -o tsv)

# Cloner le repo et installer
mkdir -p /home/cloud/app
cd /home/cloud/app
git clone https://github.com/CallmeVRM/azure-LB02 /tmp/lab
cp /tmp/lab/app/app2.js ./server.js
npm init -y
npm install express

# D√©marrer le serveur
node server.js
```

#### Sur app-vm2_b
```bash
# M√™me proc√©dure que app-vm2, mais avec :
cp /tmp/lab/app/app2_b.js ./server.js
```

#### Sur data-vm2
```bash
# Se connecter √† la VM
az network bastion ssh --name bastion --resource-group $rg \
  --auth-type password --username cloud \
  --target-resource-id $(az vm show --name data-vm2 -g $rg --query id -o tsv)

# Cloner le repo et installer
mkdir -p /home/cloud/data
cd /home/cloud/data
git clone https://github.com/CallmeVRM/azure-LB02 /tmp/lab
cp /tmp/lab/data/data2.js ./server.js
npm init -y
npm install express

# D√©marrer le serveur
node server.js
```

#### Sur data-vm2_b
```bash
# M√™me proc√©dure que data-vm2, mais avec :
cp /tmp/lab/data/data2_b.js ./server.js
```

## üîç Tests et Validation

### 1. Tester les endpoints directement (VMs individuelles)

#### Frontend VMs (port 8443)

```bash
# Frontend-VM2
curl -s http://10.1.0.20:8443/whoami | jq .
curl -s http://10.1.0.20:8443/health
curl -s http://10.1.0.20:8443/metrics | jq .

# Frontend-VM2_b
curl -s http://10.1.0.21:8443/whoami | jq .
curl -s http://10.1.0.21:8443/health
curl -s http://10.1.0.21:8443/metrics | jq .
```

#### App VMs (ports 5001, 5002)

```bash
# App-VM2 (port 5001)
curl -s http://10.2.0.20:5001/whoami | jq .
curl -s http://10.2.0.20:5001/health
curl -s http://10.2.0.20:5001/api | jq .

# App-VM2_b (port 5002)
curl -s http://10.2.0.21:5002/whoami | jq .
curl -s http://10.2.0.21:5002/health
curl -s http://10.2.0.21:5002/api | jq .
```

#### Data VMs (ports 6001, 6002)

```bash
# Data-VM2 (port 6001)
curl -s http://10.3.0.20:6001/whoami | jq .
curl -s http://10.3.0.20:6001/db | jq .
curl -s http://10.3.0.20:6001/health

# Data-VM2_b (port 6002)
curl -s http://10.3.0.21:6002/whoami | jq .
curl -s http://10.3.0.21:6002/db | jq .
curl -s http://10.3.0.21:6002/health
```

### 2. Tester les Load Balancers (distribution round-robin)

#### App Load Balancer (10.2.0.250)

```bash
# Port 5001 - Distribue vers app-vm1 et app-vm2
for i in {1..10}; do
  curl -s http://10.2.0.250:5001/whoami | jq -r '.instance'
done
# Devrait alterner entre app-1 et app-2

# Port 5002 - Devrait router vers app-vm2_b (si health probe configur√©)
for i in {1..5}; do
  curl -s http://10.2.0.250:5002/whoami | jq -r '.instance'
done
# Devrait retourner app-2_b
```

#### Data Load Balancer (10.3.0.250)

```bash
# Port 6001 - Distribue vers data-vm1 et data-vm2
for i in {1..10}; do
  curl -s http://10.3.0.250:6001/whoami | jq -r '.instance'
done
# Devrait alterner entre data-1 et data-2

# Port 6002 - Devrait router vers data-vm2_b (si health probe configur√©)
for i in {1..5}; do
  curl -s http://10.3.0.250:6002/whoami | jq -r '.instance'
done
# Devrait retourner data-2_b
```

### 3. Tester les probes depuis le frontend

Ces endpoints √©vitent les probl√®mes CORS en faisant les appels c√¥t√© serveur :

```bash
# Probes vers App Layer
curl -s http://10.1.0.20:8443/probe/app | jq .
curl -s http://10.1.0.20:8443/probe/app-health

# Probes vers Data Layer
curl -s http://10.1.0.20:8443/probe/data | jq .
curl -s http://10.1.0.20:8443/probe/data-health
```

### 4. Tester la cha√Æne compl√®te (Frontend ‚Üí App ‚Üí Data)

```bash
# Depuis le frontend, appeler l'API qui traverse toutes les couches
curl -s http://10.1.0.20:8443/api

# R√©p√©ter plusieurs fois pour voir la distribution du LB
for i in {1..10}; do
  echo "=== Requ√™te $i ==="
  curl -s http://10.1.0.20:8443/api | jq .
  sleep 1
done
```

### 5. Tester l'interface web

```bash
# Ouvrir dans le navigateur (remplacer par l'IP publique du frontend LB)
open http://<FRONTEND_PUBLIC_IP>:8443

# L'interface devrait afficher :
# - Card Frontend avec nom d'instance (frontend-2 ou frontend-2_b) et IP
# - Card App avec nom d'instance (app-1 ou app-2) et IP
# - Card Data avec nom d'instance (data-1 ou data-2) et IP
# - Badges de statut verts si tout fonctionne
# - Auto-refresh toutes les 3 secondes avec animations
```

### 6. Tester la haute disponibilit√©

#### Simuler une panne de VM

```bash
# Arr√™ter le serveur Node.js sur app-vm2
az network bastion ssh --name bastion --resource-group $rg --auth-type password --username cloud --target-resource-id $(az vm show --name app-vm2 -g $rg --query id -o tsv)

cd ~/app
sudo pkill -f server.js


#Une fois les testes finis, Red√©marrez le serveur node.
sudo nohup node server.js > server.log 2>&1 &

```

#### V√©rifier les health probes

```bash
# V√©rifier le statut des health probes sur le Load Balancer
az network lb probe show \
  --resource-group rg-loadbalancer \
  --lb-name lb-app \
  --name healthprobe-app-5001

# Voir les backends actifs
az network lb address-pool show \
  --resource-group rg-loadbalancer \
  --lb-name lb-app \
  --name bepool-app \
  --query backendIPConfigurations[].id -o table
```

### 7. Tests de charge (optionnel)

```bash
# Installer Apache Bench
sudo apt-get install apache2-utils

# Test de charge sur l'API compl√®te (10000 requ√™tes, 100 concurrentes)
ab -n 10000 -c 100 http://10.2.0.250:5001/api

# V√©rifier les m√©triques apr√®s le test
curl -s http://10.2.0.20:5001/metrics | jq .
curl -s http://10.3.0.20:6001/metrics | jq .
```

## üìä Monitoring et M√©triques

Chaque service expose un endpoint `/metrics` qui retourne :
- Nom de l'instance
- Port d'√©coute
- Uptime du processus
- Utilisation m√©moire
- Timestamp

Exemple :
```bash
curl -s http://10.1.0.20:8443/metrics | jq .
```

## üîß Maintenance

### Red√©marrer un service

```bash
# Trouver le processus Node.js
ps aux | grep node

# Tuer un processus sp√©cifique
sudo pkill -f frontend2.js  # ou app2.js, app2_b.js, data2.js, data2_b.js

# Relancer le service
cd /home/cloud
nohup node frontend2.js > frontend.log 2>&1 &
nohup node app2.js > app.log 2>&1 &
nohup node data2.js > data.log 2>&1 &
```

### Voir les logs

```bash
# Logs cloud-init (apr√®s d√©ploiement initial)
sudo tail -f /var/log/cloud-init-output.log

# Logs applicatifs (si lanc√© avec nohup)
tail -f /home/cloud/frontend.log
tail -f /home/cloud/app.log
tail -f /home/cloud/data.log

# Logs syst√®me Node.js
journalctl -u node --follow
```

### V√©rifier l'√©tat des services

```bash
# V√©rifier que le port √©coute
sudo ss -tlnp | grep 8443  # Frontend
sudo ss -tlnp | grep 5001  # App-2
sudo ss -tlnp | grep 5002  # App-2_b
sudo ss -tlnp | grep 6001  # Data-2
sudo ss -tlnp | grep 6002  # Data-2_b

# Tester les endpoints localement
curl http://localhost:8443/health  # Frontend
curl http://localhost:5001/health  # App
curl http://localhost:6001/health  # Data

# V√©rifier les processus Node.js
ps aux | grep node
```

### Mettre √† jour le code

```bash
# Se connecter √† la VM √† condition qu'un NAT au niveau du loadbalancer soit configur√©
ssh -p <port> cloud@lb-pub-ip-in

# Sauvegarder l'ancienne version
cp frontend2.js frontend2.js.bak

# Mettre √† jour depuis GitHub
cd /tmp
git clone https://github.com/CallmeVRM/azure-LB02.git
cp azure-LB02/frontend/frontend2.js /home/cloud/

# Red√©marrer le service
pkill -f frontend2.js
cd /home/cloud
nohup node frontend2.js > frontend.log 2>&1 &

# V√©rifier que √ßa fonctionne
sleep 2
curl http://localhost:8443/health
```

## üéØ Prochaines Am√©liorations Sugg√©r√©es

### 1. D√©tection Automatique des Backends
Pour d√©tecter automatiquement les nouveaux backends ajout√©s au Load Balancer :

**Option A : Polling Azure API**
```javascript
// Interroger l'API Azure toutes les 30 secondes pour d√©tecter les nouveaux backends
const { DefaultAzureCredential } = require('@azure/identity');
const { NetworkManagementClient } = require('@azure/arm-network');

async function getBackendPool() {
  const credential = new DefaultAzureCredential();
  const client = new NetworkManagementClient(credential, subscriptionId);
  const lb = await client.loadBalancers.get(resourceGroup, loadBalancerName);
  return lb.backendAddressPools[0].backendIPConfigurations;
}
```

**Option B : Health Probes Dynamiques**
```javascript
// Interroger tous les IPs d'un range et d√©tecter les r√©ponses
async function discoverBackends() {
  const range = ['10.2.0.10', '10.2.0.20', '10.2.0.6', '10.2.0.7'];
  const active = [];
  
  for (const ip of range) {
    try {
      await httpGetWithTimeout(`http://${ip}:5001/health`, 2000);
      active.push(ip);
    } catch (e) {
      // Pas actif
    }
  }
  return active;
}
```

**Option C : Service Discovery (Recommand√© pour production)**
- Utiliser **Azure Service Bus** ou **Redis** comme registre de services
- Chaque backend s'enregistre au d√©marrage
- Frontend/App interrogent le registre pour conna√Ætre les backends actifs

### 2. Persistance avec systemd

Cr√©er des services systemd pour auto-restart :

```ini
# /etc/systemd/system/frontend2.service
[Unit]
Description=Frontend-2 Node Service
After=network.target

[Service]
Type=simple
User=cloud
WorkingDirectory=/home/cloud/frontend
ExecStart=/usr/bin/node server.js
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
```

Activer :
```bash
sudo systemctl enable frontend2
sudo systemctl start frontend2
sudo systemctl status frontend2
```

### 3. Monitoring avec Prometheus

Ajouter des m√©triques Prometheus :
```javascript
const promClient = require('prom-client');
const register = new promClient.Registry();

const httpRequestDuration = new promClient.Histogram({
  name: 'http_request_duration_seconds',
  help: 'Duration of HTTP requests in seconds',
  labelNames: ['method', 'route', 'status']
});

register.registerMetric(httpRequestDuration);

app.get('/metrics', async (req, res) => {
  res.set('Content-Type', register.contentType);
  res.end(await register.metrics());
});
```

### 4. Load Balancing Intelligent

Impl√©menter un round-robin c√¥t√© client pour distribuer entre plusieurs backends :
```javascript
const backends = ['10.2.0.10:5001', '10.2.0.20:5001'];
let currentIndex = 0;

function getNextBackend() {
  const backend = backends[currentIndex];
  currentIndex = (currentIndex + 1) % backends.length;
  return backend;
}
```

### 5. Circuit Breaker Pattern

√âviter les cascades d'erreurs avec un circuit breaker :
```javascript
class CircuitBreaker {
  constructor(threshold = 5, timeout = 60000) {
    this.failures = 0;
    this.threshold = threshold;
    this.timeout = timeout;
    this.state = 'CLOSED'; // CLOSED, OPEN, HALF_OPEN
  }

  async call(fn) {
    if (this.state === 'OPEN') {
      throw new Error('Circuit breaker is OPEN');
    }
    try {
      const result = await fn();
      this.onSuccess();
      return result;
    } catch (err) {
      this.onFailure();
      throw err;
    }
  }

  onSuccess() {
    this.failures = 0;
    this.state = 'CLOSED';
  }

  onFailure() {
    this.failures++;
    if (this.failures >= this.threshold) {
      this.state = 'OPEN';
      setTimeout(() => {
        this.state = 'HALF_OPEN';
        this.failures = 0;
      }, this.timeout);
    }
  }
}
```

## üìù Notes Importantes

1. **Load Balancer Health Probes** : Assurez-vous que les health probes Azure sont configur√©s pour interroger `/health` sur chaque backend

2. **S√©curit√©** : En production, ajoutez :
   - HTTPS/TLS
   - Authentification
   - Rate limiting
   - WAF (Web Application Firewall)

3. **Performance** : Pour de meilleures performances :
   - Utilisez PM2 ou cluster mode de Node.js
   - Ajoutez du caching (Redis)
   - Utilisez HTTP/2

4. **Backup** : Configurez des snapshots r√©guliers des VMs ou utilisez Azure Backup

## üìã Tableau r√©capitulatif des VMs

| Layer | VM | IP Priv√©e | Port | Instance | Fichiers | Cloud-Init |
|-------|-----|-----------|------|----------|----------|------------|
| **Frontend** | frontend-vm2 | 10.1.0.20 | 8443 | `frontend-2` | frontend2.js, index2.html | cloud-init-frontend2.yaml |
| **Frontend** | frontend-vm2_b | 10.1.0.21 | 8443 | `frontend-2_b` | frontend2_b.js, index2_b.html | cloud-init-frontend2_b.yaml |
| **App** | app-vm2 | 10.2.0.20 | 5001 | `app-2` | app2.js | cloud-init-app2.yaml |
| **App** | app-vm2_b | 10.2.0.21 | 5002 | `app-2_b` | app2_b.js | cloud-init-app2_b.yaml |
| **Data** | data-vm2 | 10.3.0.20 | 6001 | `data-2` | data2.js | cloud-init-data2.yaml |
| **Data** | data-vm2_b | 10.3.0.21 | 6002 | `data-2_b` | data2_b.js | cloud-init-data2_b.yaml |

### Load Balancers

| LB | IP Interne | Ports | Backend Pool | Health Probe |
|----|------------|-------|--------------|--------------|
| **lb-frontend** | Public IP | 80, 8443 | frontend-vm1, vm2, vm2_b | /health |
| **lb-app** | 10.2.0.250 | 5000, 5001, 5002* | app-vm1, vm2, vm2_b* | /health |
| **lb-data** | 10.3.0.250 | 6000, 6001, 6002* | data-vm1, vm2, vm2_b* | /health |

> *Note : Les ports 5002 et 6002 doivent √™tre ajout√©s manuellement dans `infra.sh` pour les VMs `_b`.

## üÜò Troubleshooting

### 1. L'interface web ne se met pas √† jour

**Sympt√¥mes** : L'interface affiche "Loading..." ou ne rafra√Æchit pas les donn√©es

**Solutions** :
```bash
# V√©rifier que le serveur frontend √©coute sur le port 8443
ssh cloud@10.1.0.20
sudo ss -tlnp | grep 8443

# Tester les probes manuellement
curl http://10.1.0.20:8443/probe/app
curl http://10.1.0.20:8443/probe/data

# V√©rifier les logs du frontend
tail -f /home/cloud/frontend.log

# Red√©marrer le service si n√©cessaire
pkill -f frontend2.js
nohup node /home/cloud/frontend2.js > /home/cloud/frontend.log 2>&1 &
```

### 2. Les health checks Azure √©chouent

**Sympt√¥mes** : Les VMs sont marqu√©es "Unhealthy" dans le backend pool

**Solutions** :
```bash
# V√©rifier que l'endpoint /health r√©pond localement
ssh cloud@10.2.0.20
curl http://localhost:5001/health
# Devrait retourner "OK"

# V√©rifier que le port est accessible depuis une autre VM
# Depuis frontend-vm2
curl http://10.2.0.20:5001/health

# V√©rifier la configuration du health probe Azure
az network lb probe show \
  --resource-group rg-loadbalancer \
  --lb-name lb-app \
  --name healthprobe-app-5001

# V√©rifier les NSG (Network Security Groups)
az network nsg rule list \
  --resource-group rg-loadbalancer \
  --nsg-name nsg-app \
  --output table
```

### 3. Timeout sur les requ√™tes (5 secondes)

**Sympt√¥mes** : Erreurs "Error: timeout" dans les logs ou l'interface

**Solutions** :
```bash
# V√©rifier la latence r√©seau
ping 10.2.0.250
ping 10.3.0.250

# Tester manuellement la cha√Æne compl√®te
time curl http://10.1.0.20:8443/api

# V√©rifier la charge CPU/RAM des VMs
ssh cloud@10.2.0.20
top
# Si CPU > 80%, consid√©rer un redimensionnement de VM

# Augmenter le timeout dans le code si n√©cessaire (actuellement 5000ms)
# Dans frontend2.js, app2.js, app2_b.js : modifier httpGetWithTimeout
```

### 4. Cloud-init n'a pas d√©marr√© le serveur

**Sympt√¥mes** : VM cr√©√©e mais serveur Node.js non actif

**Solutions** :
```bash
# V√©rifier les logs cloud-init
ssh cloud@<VM-IP>
sudo tail -100 /var/log/cloud-init-output.log

# Chercher des erreurs sp√©cifiques
sudo cat /var/log/cloud-init-output.log | grep -i error

# Erreurs courantes :
# - "cannot stat frontend2_b.js" ‚Üí Fichiers non commit√©s sur GitHub
# - "npm ERR!" ‚Üí Probl√®me d'installation de d√©pendances
# - "Address already in use" ‚Üí Port d√©j√† occup√©

# Red√©marrer manuellement si n√©cessaire
cd /home/cloud
git clone https://github.com/CallmeVRM/azure-LB02.git
cp azure-LB02/app/app2.js ./
npm init -y
npm install express
nohup node app2.js > app.log 2>&1 &
```

### 5. Load Balancer ne distribue pas les requ√™tes

**Sympt√¥mes** : Toutes les requ√™tes vont vers la m√™me VM

**Solutions** :
```bash
# Tester la distribution LB
for i in {1..20}; do
  curl -s http://10.2.0.250:5001/whoami | jq -r '.instance'
done
# Devrait alterner entre app-1 et app-2

# V√©rifier le backend pool
az network lb address-pool show \
  --resource-group rg-loadbalancer \
  --lb-name lb-app \
  --name bepool-app \
  --query backendIPConfigurations[].id -o table

# V√©rifier que les VMs sont "Healthy"
az network nic show-effective-route-table \
  --resource-group rg-loadbalancer \
  --name app-vm2VMNic \
  --output table

# V√©rifier la r√®gle de load balancing
az network lb rule show \
  --resource-group rg-loadbalancer \
  --lb-name lb-app \
  --name rule-app-5001
```

### 6. VMs `_b` ne re√ßoivent pas de trafic

**Sympt√¥mes** : app-vm2_b ou data-vm2_b ne r√©pondent jamais

**Solutions** :
```bash
# V√©rifier que les health probes pour les ports 5002/6002 existent
az network lb probe list \
  --resource-group rg-loadbalancer \
  --lb-name lb-app \
  --output table

# Si absents, les ajouter dans infra.sh :
# az network lb probe create \
#   --resource-group rg-loadbalancer \
#   --lb-name lb-app \
#   --name healthprobe-app-5002 \
#   --protocol tcp \
#   --port 5002 \
#   --interval 5 \
#   --threshold 2

# Cr√©er √©galement les r√®gles de load balancing pour les ports 5002/6002
# Voir TEST-LB-APP-DATA.md pour les commandes compl√®tes
```

### 7. Erreur "Cannot find module 'express'"

**Sympt√¥mes** : Erreur au d√©marrage du serveur Node.js

**Solutions** :
```bash
# Se connecter √† la VM
ssh cloud@<VM-IP>

# Installer express manuellement
cd /home/cloud
npm init -y
npm install express

# Red√©marrer le serveur
nohup node app2.js > app.log 2>&1 &
```

## ÔøΩ Structure des fichiers du projet

```
azure-LB02/
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îú‚îÄ‚îÄ frontend2.js           # Serveur frontend principal (port 8443)
‚îÇ   ‚îú‚îÄ‚îÄ frontend2_b.js          # Serveur frontend test (port 8443)
‚îÇ   ‚îú‚îÄ‚îÄ index2.html             # Interface web moderne (frontend-2)
‚îÇ   ‚îú‚îÄ‚îÄ index2_b.html           # Interface web test (frontend-2_b)
‚îÇ   ‚îú‚îÄ‚îÄ cloud-init-frontend2.yaml    # D√©ploiement automatique frontend-2
‚îÇ   ‚îî‚îÄ‚îÄ cloud-init-frontend2_b.yaml  # D√©ploiement automatique frontend-2_b
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ app2.js                 # Serveur app principal (port 5001)
‚îÇ   ‚îú‚îÄ‚îÄ app2_b.js               # Serveur app test (port 5002)
‚îÇ   ‚îú‚îÄ‚îÄ cloud-init-app2.yaml   # D√©ploiement automatique app-2
‚îÇ   ‚îî‚îÄ‚îÄ cloud-init-app2_b.yaml # D√©ploiement automatique app-2_b
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ data2.js                # Serveur data principal (port 6001)
‚îÇ   ‚îú‚îÄ‚îÄ data2_b.js              # Serveur data test (port 6002)
‚îÇ   ‚îú‚îÄ‚îÄ cloud-init-data2.yaml  # D√©ploiement automatique data-2
‚îÇ   ‚îî‚îÄ‚îÄ cloud-init-data2_b.yaml # D√©ploiement automatique data-2_b
‚îú‚îÄ‚îÄ STACK-8443-README.md        # Documentation compl√®te (ce fichier)
‚îú‚îÄ‚îÄ TEST-LB-APP-DATA.md         # Guide de test pour les VMs _b
‚îú‚îÄ‚îÄ FRONTEND-2B-README.md       # Guide sp√©cifique frontend-vm2_b
‚îî‚îÄ‚îÄ infra.sh                    # Script de d√©ploiement infrastructure Azure
```

## üîó Liens vers documentation compl√©mentaire

- **[TEST-LB-APP-DATA.md](./TEST-LB-APP-DATA.md)** : Guide d√©taill√© pour d√©ployer et tester les VMs `_b` (app-vm2_b, data-vm2_b)
- **[FRONTEND-2B-README.md](./FRONTEND-2B-README.md)** : Documentation sp√©cifique pour frontend-vm2_b
- **[infra.sh](./infra.sh)** : Script Bash pour cr√©er toute l'infrastructure Azure (VNet, Subnets, NSG, Load Balancers, Health Probes)

## ÔøΩüìö Ressources externes

- [Express.js Documentation](https://expressjs.com/) - Framework web Node.js
- [Azure Load Balancer Documentation](https://docs.microsoft.com/azure/load-balancer/) - Service de load balancing Azure
- [Node.js Best Practices](https://github.com/goldbergyoni/nodebestpractices) - Guide des bonnes pratiques Node.js
- [Azure Cloud-Init Guide](https://docs.microsoft.com/azure/virtual-machines/linux/using-cloud-init) - Automatisation du provisioning VM
- [MDN - CSS Grid](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Grid_Layout) - Layout utilis√© dans l'interface web

## üéì Concepts Azure couverts

- ‚úÖ **Azure Load Balancer** (Public et Internal)
- ‚úÖ **Backend Address Pools** avec plusieurs VMs
- ‚úÖ **Health Probes** sur endpoint `/health`
- ‚úÖ **Load Balancing Rules** avec round-robin
- ‚úÖ **Network Security Groups (NSG)** avec r√®gles de trafic
- ‚úÖ **Azure Virtual Network** avec subnets segment√©s
- ‚úÖ **Cloud-Init** pour provisioning automatique
- ‚úÖ **Static Private IP addressing**

## üöÄ Commandes rapides (Quick Reference)

### Tests rapides

```bash
# Tester frontend
curl http://10.1.0.20:8443/whoami

# Tester app via LB
curl http://10.2.0.250:5001/whoami

# Tester data via LB
curl http://10.3.0.250:6001/whoami

# Tester cha√Æne compl√®te
curl http://10.1.0.20:8443/api
```

### V√©rifications rapides

```bash
# Voir les processus Node.js
ps aux | grep node

# Voir les ports en √©coute
sudo ss -tlnp | grep node

# Tester health checks
curl http://localhost:8443/health
curl http://localhost:5001/health
curl http://localhost:6001/health
```

### Red√©marrage rapide

```bash
# Red√©marrer tous les services

cd /home/cloud/dossier
sudo pkill -f server.js
nohup node server.js > server.log 2>&1 &
nohup node app2.js > app.log 2>&1 &
nohup node data2.js > data.log 2>&1 &
```

---

**Auteur** : VRM  
**Projet** : Azure Load Balancer Lab - Stack 8443  
**Repository** : [github.com/CallmeVRM/azure-LB02](https://github.com/CallmeVRM/azure-LB02)  
**Date** : 2025  
**Version** : 2.0 (avec VMs de test _b)

---

## ‚ú® Changelog

### Version 2.0 (Actuelle)
- ‚úÖ Ajout des VMs de test `_b` (frontend-vm2_b, app-vm2_b, data-vm2_b)
- ‚úÖ Interface web moderne avec cards, gradients et animations
- ‚úÖ Timeouts de 5 secondes sur toutes les requ√™tes HTTP
- ‚úÖ Endpoints `/metrics` pour monitoring
- ‚úÖ Cloud-init pour d√©ploiement automatique
- ‚úÖ Documentation compl√®te (STACK-8443, TEST-LB-APP-DATA, FRONTEND-2B)
- ‚úÖ Support multi-backend avec ports alternatifs (5002, 6002)

### Version 1.0 (Initiale)
- D√©ploiement basique frontend-vm2, app-vm2, data-vm2
- Interface HTML simple
- Scripts Node.js avec Express
